{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gluonnlp as nlp\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from gluonnlp.data import SentencepieceTokenizer\n",
    "\n",
    "bertmodel, vocab = get_pytorch_kobert_model()\n",
    "\n",
    "# train, test dataset 불러오기.\n",
    "dataset_train = nlp.data.TSVDataset('/Users/waterpurifier/Comment_train.txt',encoding='utf-8', field_indices=[2,3], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset('/Users/waterpurifier/Comment_test.txt',encoding='utf-8', field_indices=[2,3], num_discard_samples=1)\n",
    "\n",
    "token_path = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(token_path, vocab, lower=False)\n",
    "\n",
    "# BERTDataset\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len, pad, pair):\n",
    "\n",
    "        transform = nlp.data.BERTSentenceTransform(bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "        #self.adsf = transform(dataset)[0]\n",
    "        #print(self.adsf[0])\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "        \n",
    "        # 기존의 방식에서 model에 적용할 때 data type이 안맞아서 <list> (int) 로 맞췄음.\n",
    "        self.sentences = [i[0].tolist() for i in self.sentences]\n",
    "        self.labels = [i.tolist() for i in self.labels]\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n",
    "    \n",
    "\n",
    "max_len = 50\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_long_variable(w2i_ls):\n",
    "    return Variable(torch.LongTensor(w2i_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = convert_to_long_variable(data_train.sentences)\n",
    "y_train = convert_to_long_variable(data_train.labels)\n",
    "\n",
    "x_test = convert_to_long_variable(data_test.sentences)\n",
    "y_test = convert_to_long_variable(data_test.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 케라스 모델의 CNN Summary\n",
    "# embedding dimmension = 128\n",
    "# dropout = (0.5, 0.8)\n",
    "# filter size = 64\n",
    "# model input = max_len : 50\n",
    "# z = Embedding(vocab_size, embedding dimmension, input_length = 50, )\n",
    "# z = dropout(0.5)\n",
    "\n",
    "# conv 1d = conv1d(filter = 64, kernel 3, 4, 5, 6, padding = valid, activation = Relu)\n",
    "# Global Max Pooling 1D\n",
    "# Flatten\n",
    "# dropout 0.8\n",
    "# hidden layer =128#\n",
    "# model output = sigmoid 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_text(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_words, embed_size, pad_index, hid_size, drop_rate, kernel_size_ls, num_filter, n_class):\n",
    "        super(CNN_text, self).__init__()\n",
    "        \n",
    "        self.pad_index = pad_index              # 단어 embedding 과정에서 제외시킬 padding token\n",
    "        self.embed_size = embed_size            # 임베딩 차원의 크기\n",
    "        self.hid_size = hid_size                # 히든 레이어 갯수\n",
    "        self.drop_rate = drop_rate              # 드롭아웃 비율\n",
    "        self.num_filter = num_filter            # 필터의 갯수 \n",
    "        self.kernel_size_ls = kernel_size_ls    # 각기 다른 필터 사이즈가 담긴 리스트\n",
    "        self.num_kernel = len(kernel_size_ls)   # 필터 사이즈의 종류 수\n",
    "        self.n_class = n_class                  # 카테고리 갯수\n",
    "        \n",
    "        self.embed = nn.Embedding(\n",
    "            num_embeddings=n_words, \n",
    "            embedding_dim=embed_size,\n",
    "            padding_idx=self.pad_index\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # kernel size는 (n-gram, embed_size)이다.\n",
    "        # 커널의 열(column)의 크기는 embed_size와 일치하므로, 단어 임베딩 벡터를 모두 커버한다.\n",
    "        # 따라서, n의 row 크기를 갖는 커널은 한번에 n개의 단어를 커버하는 n-gram 커널이라고 볼 수 있다.\n",
    "        self.convs = nn.ModuleList([nn.Conv1d(1, num_filter, (kernel_size, embed_size)) for kernel_size in kernel_size_ls])\n",
    "        \n",
    "        self.lin = nn.Sequential(\n",
    "            nn.Linear(self.num_kernel*num_filter, hid_size), nn.ReLU(), nn.Dropout(drop_rate),\n",
    "            nn.Linear(hid_size, n_class),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embed = self.embed(x) # batch_size x max_length x embed_size\n",
    "        embed.unsqueeze_(1)       # batch_size, 1, max_length, embed_size : convolution을 위해 4D로 차원을 조절\n",
    "        \n",
    "        # convolution\n",
    "        conved = [conv(embed).squeeze(3) for conv in self.convs] # [batch_size, num_filter, max_length -kernel_size +1]\n",
    "        \n",
    "        # max_pooling\n",
    "        pooled = [F.max_pool1d(conv, (conv.size(2))).squeeze(2) for conv in conved] # [batch_size, num_kernel, num_filter]\n",
    "            \n",
    "        # dropout\n",
    "        dropouted = [F.dropout(pool, self.drop_rate) for pool in pooled]\n",
    "        \n",
    "        # concatenate\n",
    "        concated = torch.cat(dropouted, dim = 1) # [batch_size, num_kernel * num_filter]\n",
    "        \n",
    "        \n",
    "        logit = self.lin(m(concated))\n",
    "        \n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_words' : vocab_size,        # 고유한 단어 토큰의 갯수\n",
    "    'embed_size' : 128,                # 임베딩 차원의 크기\n",
    "    'pad_index' : max_len,  # 패딩 토큰\n",
    "    'hid_size' : 128,                  # 히든 레이어 갯수\n",
    "    'drop_rate' : 0.5,                 # 드롭아웃 비율          (원문에서는 0.5를 사용)\n",
    "    'kernel_size_ls' : [2,3,4,5],      # 커널 사이즈 리스트        (원문애서는 3,4,5를 사용)\n",
    "    'num_filter' : 32,                 # 각 사이즈 별 커널 갯수 (원문에서는 100을 사용)\n",
    "    'n_class' : 1,                  # 카테고리 갯수\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_text(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch : 1,  loss : 0.25065280636772513,  accuracy :0.645\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 1, Test Loss : 0.000 , Test Accuracy : 0.696\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 2,  loss : 0.2235829319106415,  accuracy :0.793\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 2, Test Loss : 0.000 , Test Accuracy : 0.805\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 3,  loss : 0.1922169120516628,  accuracy :0.812\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 3, Test Loss : 0.000 , Test Accuracy : 0.836\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 4,  loss : 0.17009162961039692,  accuracy :0.852\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 4, Test Loss : 0.000 , Test Accuracy : 0.844\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Train epoch : 5,  loss : 0.15286990068852901,  accuracy :0.852\n",
      "=================================================================================================\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n",
      "Test Epoch : 5, Test Loss : 0.000 , Test Accuracy : 0.858\n",
      "*************************************************************************************************\n",
      "*************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "lr = 0.0003\n",
    "batch_size = 128*2\n",
    "\n",
    "train_idx = np.arange(x_train.size(0))\n",
    "test_idx = np.arange(x_test.size(0))\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr) # 원문에서는 Adadelta 알고리즘을 사용\n",
    "#criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "m = nn.Sigmoid()\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "train_loss_ls = []\n",
    "test_loss_ls = []\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    \n",
    "    # input 데이터 순서 섞기\n",
    "    random.shuffle(train_idx)\n",
    "    x_train = x_train[train_idx]\n",
    "    y_train = y_train[train_idx]\n",
    "    train_loss = 0\n",
    "    test_loss = 0\n",
    "    for start_idx, end_idx in zip(range(0, x_train.size(0), batch_size),\n",
    "                                  range(batch_size, x_train.size(0)+1, batch_size)):\n",
    "        \n",
    "        x_batch = x_train[start_idx : end_idx]\n",
    "        y_batch = y_train[start_idx : end_idx].float()\n",
    "        \n",
    "        scores = model(x_batch)\n",
    "        #predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        predict = []\n",
    "        for i in m(scores):\n",
    "            if i > 0.5: \n",
    "                predict.append(1)\n",
    "            else:\n",
    "                predict.append(0)\n",
    "        predict = convert_to_long_variable(predict)\n",
    "        acc = (predict == y_batch).sum().item() / batch_size\n",
    "        \n",
    "        loss = criterion(m(scores), y_batch)\n",
    "        train_loss += loss.item()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Train epoch : %s,  loss : %s,  accuracy :%.3f'%(epoch+1, train_loss / batch_size, acc))\n",
    "    print('=================================================================================================')\n",
    "    \n",
    "    \n",
    "    train_loss_ls.append(acc)\n",
    "    if (epoch+1) % 1 == 0:\n",
    "        model.eval()\n",
    "        scores = model(x_test)\n",
    "        #predict = F.softmax(scores, dim=1).argmax(dim = 1)\n",
    "        predict = []\n",
    "        for i in m(scores):\n",
    "            if i > 0.5: \n",
    "                predict.append(1)\n",
    "            else:\n",
    "                predict.append(0)\n",
    "        predict = convert_to_long_variable(predict)\n",
    "        \n",
    "        acc = (predict == y_test.long()).sum().item() / y_test.size(0)\n",
    "        loss = criterion(m(scores), y_test.float())\n",
    "        test_loss += loss.item()\n",
    "        test_loss_ls.append(test_loss)\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')\n",
    "        print('Test Epoch : %s, Test Loss : %.03f , Test Accuracy : %.03f'%(epoch+1, test_loss/y_test.size(0), acc))\n",
    "        print('*************************************************************************************************')\n",
    "        print('*************************************************************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-463-354737640d59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'뭐어쩌라구욧!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mexplanation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplain_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'뭐 어쩌라구욧'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.02/lib/python3.7/site-packages/lime/lime_text.py\u001b[0m in \u001b[0;36mexplain_instance\u001b[0;34m(self, text_instance, classifier_fn, labels, top_labels, num_features, num_samples, distance_metric, model_regressor)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistances\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m                 \u001b[0mmodel_regressor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_regressor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 feature_selection=self.feature_selection)\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret_exp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-2020.02/lib/python3.7/site-packages/lime/lime_base.py\u001b[0m in \u001b[0;36mexplain_instance_with_data\u001b[0;34m(self, neighborhood_data, neighborhood_labels, distances, label, num_features, feature_selection, model_regressor)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistances\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mlabels_column\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneighborhood_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         used_features = self.feature_selection(neighborhood_data,\n\u001b[1;32m    184\u001b[0m                                                \u001b[0mlabels_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "def predict_fn(text):\n",
    "    Text = text[0]\n",
    "    text = BERTDataset([[Text, '1']], 0, 1, tok, 50, True, False)\n",
    "    text = convert_to_long_variable(text.sentences[0])\n",
    "    return m(model(text.unsqueeze(0)))\n",
    "\n",
    "explainer = LimeTextExplainer()\n",
    "predict_fn('뭐어쩌라구욧!')\n",
    "explanation = explainer.explain_instance('뭐 어쩌라구욧', predict_fn, num_features = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 2145, 7818, 5760, 862, 7318, 6999, 517, 463, 632, 517, 463, 633, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "['ㅂ2염', '0']\n",
      "tensor([[-3.4441]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0.0309]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_text = '뭐하자는 거지요~>~?'\n",
    "#data_train = ElectraDataset(dataset_train['comment'], dataset_train['label'], max_len)\n",
    "tests = BERTDataset([[test_text, '1']], 0, 1, tok, 50, True, False)\n",
    "print(tests.sentences)\n",
    "tests = convert_to_long_variable(tests.sentences[0])\n",
    "#tests.unsqueeze(0).shape\n",
    "#print(predict_fn(tests))\n",
    "predict = model(tests.unsqueeze(0))\n",
    "print(dataset_train[1])\n",
    "print(predict)\n",
    "print(m(predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6489]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LngPurifier",
   "language": "python",
   "name": "lngpurifier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
